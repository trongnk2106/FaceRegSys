{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "013edd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hoang\\anaconda3\\envs\\ComputerVision\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import dataset\n",
    "import argparse\n",
    "from models.model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "017fc513",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m hehe\u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mFaceImageFolderDataset(root\u001b[38;5;241m=\u001b[39m \u001b[43margs\u001b[49m\u001b[38;5;241m.\u001b[39mfolderdataset_dir)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "hehe= dataset.FaceImageFolderDataset(root= args.folderdataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7689f1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "la=10\n",
    "ua=110\n",
    "lm=0.45\n",
    "um=0.8\n",
    "lg=35\n",
    "\n",
    "# settings\n",
    "MODEL_ARC='alexnet'\n",
    "OUTPUT='test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957eb317",
   "metadata": {},
   "outputs": [],
   "source": [
    "python -u trainer.py --backbone alexnet --folderdataset_dir datasets/raw --pretrained_backbone True --workers 1 --epochs 25 --start-epoch 0 --batch-size 1 --embedding-size 512 --last-fc-size 2 --print-freq 2 --pth-save-fold test --pth-save-epoch 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "44cd1a96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m=> parse the args ...\u001b[0m\n",
      "{'arc_scale': 64,\n",
      " 'backbone': 'alexnet',\n",
      " 'batch_size': 1,\n",
      " 'embedding_size': 512,\n",
      " 'epochs': 25,\n",
      " 'folderdataset_dir': 'datasets/raw',\n",
      " 'l_a': 10,\n",
      " 'l_margin': 0.45,\n",
      " 'lambda_g': 20,\n",
      " 'last_fc_size': 2,\n",
      " 'lr': 0.1,\n",
      " 'lr_drop_epoch': [30, 60, 90],\n",
      " 'lr_drop_ratio': 0.1,\n",
      " 'momentum': 0.9,\n",
      " 'pretrained_backbone': 'True',\n",
      " 'print_freq': 2,\n",
      " 'pth_save_epoch': 1,\n",
      " 'pth_save_fold': 'test',\n",
      " 'start_epoch': 0,\n",
      " 'train_list': '',\n",
      " 'u_a': 110,\n",
      " 'u_margin': 0.8,\n",
      " 'vis_mag': 1,\n",
      " 'weight_decay': 0.0001,\n",
      " 'workers': 1}\n",
      "\u001b[31mmin lambda g is 22.586666666666673, currrent lambda is 20\u001b[0m\n",
      "\u001b[32m=> torch version : 1.12.1\u001b[0m\n",
      "\u001b[32m=> ngpus : 1\u001b[0m\n",
      "\u001b[32m=> modeling the network ...\u001b[0m\n",
      "\u001b[32m=> building the oprimizer ...\u001b[0m\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    foreach: None\n",
      "    lr: 0.1\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0.0001\n",
      ")\n",
      "\u001b[32m=> building the dataloader ...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"T:\\Git_repo\\Face_Recognition\\face_embedding\\trainer.py\", line 265, in <module>\n",
      "    main(args)\n",
      "  File \"T:\\Git_repo\\Face_Recognition\\face_embedding\\trainer.py\", line 110, in main\n",
      "    main_worker(ngpus_per_node, args)\n",
      "  File \"T:\\Git_repo\\Face_Recognition\\face_embedding\\trainer.py\", line 134, in main_worker\n",
      "    train_loader = dataset.train_loader(args)\n",
      "  File \"T:\\Git_repo\\Face_Recognition\\face_embedding\\datasets\\dataset.py\", line 119, in train_loader\n",
      "    print(hehe)\n",
      "NameError: name 'hehe' is not defined\n"
     ]
    }
   ],
   "source": [
    "python -u trainer.py --backbone {MODEL_ARC} --folderdataset_dir datasets/raw --pretrained_backbone True --workers 1 --epochs 25 --start-epoch 0 --batch-size 1 --embedding-size 512 --last-fc-size 2 --print-freq 2 --pth-save-fold {OUTPUT} --pth-save-epoch 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e0c305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--backbone', default='iresnet18', type=str,\n",
    "                    help='backbone architechture')\n",
    "parser.add_argument('--pretrained_backbone', default=False, type=str,\n",
    "                    help='Use pretrain backbone')\n",
    "parser.add_argument('--resume', default=None, type=str, metavar='PATH',\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--folderdataset_dir', default='datasets/raw', type=str,\n",
    "                    help='Path to Face Image Folder Dataset')\n",
    "parser.add_argument('--feat_list', default='datasets/face_embdding.txt', type=str,\n",
    "                    help='Path for saveing features file')\n",
    "parser.add_argument('-j', '--workers', default=2, type=int, metavar='N',\n",
    "                    help='number of data loading workers (default: 4)')\n",
    "parser.add_argument('-b', '--batch-size', default=256, type=int, metavar='N',\n",
    "                    help='mini-batch size (default: 256), this is the total '\n",
    "                    'batch size of all GPUs on the current node when '\n",
    "                    'using Data Parallel or Distributed Data Parallel')\n",
    "parser.add_argument('--embedding-size', default=512, type=int,\n",
    "                    help='The embedding feature size')\n",
    "parser.add_argument('--cpu-mode', action='store_true', help='Use the CPU.')\n",
    "\n",
    "args=parser.parse_args(''.split())\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model= build_backbone(args)\n",
    "\n",
    "if not args.cpu_mode:\n",
    "    model = model.cuda()\n",
    "    device=\"cuda\"\n",
    "else:\n",
    "    device=\"cpu\"\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((112, 112)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0., 0., 0.],\n",
    "                                     std=[1., 1., 1.]),\n",
    "])    \n",
    "\n",
    "data_inf = dataset.FaceImageFolderDataset(root= args.folderdataset_dir, transform = transform)\n",
    "\n",
    "dataloader_inf=DataLoader(data_inf, \n",
    "                          batch_size= args.batch_size, \n",
    "                          num_workers= args.workers,\n",
    "                          pin_memory=False,\n",
    "                          shuffle=False,\n",
    ")\n",
    "\n",
    "label_map_path= os.path.join(\"/\".join(args.feat_list.split('/')[:-1]),'label_map.json')\n",
    "cprint('=> starting face embdding...', 'green')\n",
    "cprint('=> embdding feature will be saved into {}'.format(args.feat_list))\n",
    "cprint('=> mapping label will be saved into {}'.format(label_map_path))\n",
    "\n",
    "with open(label_map_path,'w') as f:\n",
    "    json.dump(data_inf.get_label_map(), f)\n",
    "    \n",
    "model.eval()    \n",
    "file= open(args.feat_list, 'w')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (imgs, labels) in enumerate(dataloader_inf):\n",
    "        print(len(imgs))\n",
    "        imgs=imgs.to(device)\n",
    "        embedding_feats = model(imgs)\n",
    "        embedding_feats = embedding_feats.data.cpu().numpy()\n",
    "        \n",
    "        for feat, label in zip(embedding_feats, labels):\n",
    "            file.write('{} '.format(label))\n",
    "            for r in feat:\n",
    "                file.write('{} '.format(r))\n",
    "            file.write('\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb1903c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m=> starting face embdding...\u001b[0m\n",
      "=> embdding feature will be saved into datasets/face_embdding.txt\u001b[0m\n",
      "=> mapping label will be saved into datasets\\label_map.json\u001b[0m\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "if not args.cpu_mode:\n",
    "    model = model.cuda()\n",
    "    device=\"cuda\"\n",
    "else:\n",
    "    device=\"cpu\"\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((112, 112)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0., 0., 0.],\n",
    "                                     std=[1., 1., 1.]),\n",
    "])    \n",
    "\n",
    "data_inf = dataset.FaceImageFolderDataset(root= args.folderdataset_dir, transform = transform)\n",
    "\n",
    "dataloader_inf=DataLoader(data_inf, \n",
    "                          batch_size= args.batch_size, \n",
    "                          num_workers= args.workers,\n",
    "                          pin_memory=False,\n",
    "                          shuffle=False,\n",
    ")\n",
    "\n",
    "label_map_path= os.path.join(\"/\".join(args.feat_list.split('/')[:-1]),'label_map.json')\n",
    "cprint('=> starting face embdding...', 'green')\n",
    "cprint('=> embdding feature will be saved into {}'.format(args.feat_list))\n",
    "cprint('=> mapping label will be saved into {}'.format(label_map_path))\n",
    "\n",
    "with open(label_map_path,'w') as f:\n",
    "    json.dump(data_inf.get_label_map(), f)\n",
    "    \n",
    "model.eval()    \n",
    "file= open(args.feat_list, 'w')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (imgs, labels) in enumerate(dataloader_inf):\n",
    "        print(len(imgs))\n",
    "        imgs=imgs.to(device)\n",
    "        embedding_feats = model(imgs)\n",
    "        embedding_feats = embedding_feats.data.cpu().numpy()\n",
    "        \n",
    "        for feat, label in zip(embedding_feats, labels):\n",
    "            file.write('{} '.format(label))\n",
    "            for r in feat:\n",
    "                file.write('{} '.format(r))\n",
    "            file.write('\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bc6ce82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "\n",
    "import cv2\n",
    "from imutils.video import VideoStream\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e384f65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--backbone', default='iresnet18', type=str,\n",
    "                    help='backbone architechture')\n",
    "parser.add_argument('--pretrained_backbone', default=False, type=str,\n",
    "                    help='Use pretrain backbone')\n",
    "parser.add_argument('--resume', default=None, type=str, metavar='PATH',\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--feat_list', default='datasets/face_embdding.txt', type=str,\n",
    "                    help='Path for saveing features file')\n",
    "parser.add_argument('--label_map', default='datasets/label_map.json', type=str,\n",
    "                    help='Path for saveing label mapping dictionary')\n",
    "parser.add_argument('-j', '--workers', default=2, type=int, metavar='N',\n",
    "                    help='number of data loading workers (default: 4)')\n",
    "parser.add_argument('-b', '--batch-size', default=512, type=int, metavar='N',\n",
    "                    help='mini-batch size (default: 256), this is the total '\n",
    "                    'batch size of all GPUs on the current node when '\n",
    "                    'using Data Parallel or Distributed Data Parallel')\n",
    "parser.add_argument('--embedding-size', default=512, type=int,\n",
    "                    help='The embedding feature size')\n",
    "parser.add_argument('--cpu-mode', action='store_true', help='Use the CPU.')\n",
    "\n",
    "args=parser.parse_args(''.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "24949ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_embdding_feature(args):\n",
    "    label= []\n",
    "    feat= []\n",
    "    with open(args.feat_list, 'r') as f:\n",
    "        file= f.read()\n",
    "        for line in file.split('\\n')[:-1]:\n",
    "            arr=line.split()\n",
    "            label.append(arr[0])\n",
    "            feat.append(np.array(arr[1:]))\n",
    "            \n",
    "    return np.array(feat), np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b2bd023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;, probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;, probability=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear', probability=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat, label= get_embdding_feature(args)\n",
    "clf= SVC(kernel='linear', probability=True)\n",
    "clf.fit(feat, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6186801a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(args.label_map, 'r') as f:\n",
    "        label_map= json.load(f)\n",
    "except:\n",
    "    label_map=None\n",
    "    \n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToPILImage(),\n",
    "    torchvision.transforms.Resize((112, 112)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0., 0., 0.],\n",
    "                                     std=[1., 1., 1.]),\n",
    "])    \n",
    "cap  = VideoStream(src=0).start()\n",
    "while(cap.isOpened()):\n",
    "    frame = cap.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    face = frame\n",
    "    face = transform(face).to(device).unsqueeze(0)\n",
    "\n",
    "    _emb_feat = model(face)\n",
    "    _emb_feat= _emb_feat.squeeze().data.cpu().numpy()\n",
    "\n",
    "    _pred = clf.predict_proba(_emb_feat.reshape(1,-1))\n",
    "    _label = np.argmax(_pred, axis=1)\n",
    "    print(label_map[str(_label[0])])\n",
    "\n",
    "\n",
    "    cv2.imshow('Face Identification', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4a9353f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'backbone': 'iresnet18',\n",
      " 'batch_size': 256,\n",
      " 'cpu_mode': False,\n",
      " 'embedding_size': 512,\n",
      " 'feat_list': 'datasets/raw/face_embdding.txt',\n",
      " 'folderdataset_dir': 'datasets/raw',\n",
      " 'pretrained_backbone': False,\n",
      " 'resume': None,\n",
      " 'workers': 2}\n",
      "\u001b[32m=> starting face embdding...\u001b[0m\n",
      "=> embdding feature will be saved into datasets/raw/face_embdding.txt\u001b[0m\n",
      "=> mapping label will be saved into datasets/raw\\label_map.json\u001b[0m\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"C:\\Users\\hoang\\anaconda3\\envs\\computervision\\lib\\multiprocessing\\spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"C:\\Users\\hoang\\anaconda3\\envs\\computervision\\lib\\multiprocessing\\spawn.py\", line 125, in _main\n",
      "    prepare(preparation_data)\n",
      "  File \"C:\\Users\\hoang\\anaconda3\\envs\\computervision\\lib\\multiprocessing\\spawn.py\", line 236, in prepare\n",
      "    _fixup_main_from_path(data['init_main_from_path'])\n",
      "  File \"C:\\Users\\hoang\\anaconda3\\envs\\computervision\\lib\\multiprocessing\\spawn.py\", line 287, in _fixup_main_from_path\n",
      "    main_content = runpy.run_path(main_path,\n",
      "  File \"C:\\Users\\hoang\\anaconda3\\envs\\computervision\\lib\\runpy.py\", line 269, in run_path\n",
      "    return _run_module_code(code, init_globals, run_name,\n",
      "  File \"C:\\Users\\hoang\\anaconda3\\envs\\computervision\\lib\\runpy.py\", line 96, in _run_module_code\n",
      "    _run_code(code, mod_globals, init_globals,\n",
      "  File \"C:\\Users\\hoang\\anaconda3\\envs\\computervision\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"T:\\Git_repo\\Face_Recognition\\face_embedding\\embdded_feat.py\", line 6, in <module>\n",
      "    import torchvision\n",
      "  File \"C:\\Users\\hoang\\anaconda3\\envs\\computervision\\lib\\site-packages\\torchvision\\__init__.py\", line 4, in <module>\n",
      "    import torch\n",
      "  File \"C:\\Users\\hoang\\anaconda3\\envs\\computervision\\lib\\site-packages\\torch\\__init__.py\", line 129, in <module>\n",
      "    raise err\n",
      "OSError: [WinError 1455] The paging file is too small for this operation to complete. Error loading \"C:\\Users\\hoang\\anaconda3\\envs\\computervision\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.\n"
     ]
    }
   ],
   "source": [
    "!python embdded_feat.py --backbone iresnet18 --folderdataset_dir datasets/raw --feat_list datasets/raw/face_embdding.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c8e605",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
